{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category map: {1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 27: 25, 28: 26, 31: 27, 32: 28, 33: 29, 34: 30, 35: 31, 36: 32, 37: 33, 38: 34, 39: 35, 40: 36, 41: 37, 42: 38, 43: 39, 44: 40, 46: 41, 47: 42, 48: 43, 49: 44, 50: 45, 51: 46, 52: 47, 53: 48, 54: 49, 55: 50, 56: 51, 57: 52, 58: 53, 59: 54, 60: 55, 61: 56, 62: 57, 63: 58, 64: 59, 65: 60, 67: 61, 70: 62, 72: 63, 73: 64, 74: 65, 75: 66, 76: 67, 77: 68, 78: 69, 79: 70, 80: 71, 81: 72, 82: 73, 84: 74, 85: 75, 86: 76, 87: 77, 88: 78, 89: 79, 90: 80}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing COCO images: 100%|██████████| 8000/8000 [11:15:38<00:00,  5.07s/it]      \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from skimage.draw import polygon\n",
    "\n",
    "def create_multiclass_mask(image_info, annotations, mask_output_dir, category_map):\n",
    "    height, width = image_info['height'], image_info['width']\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    image_id = image_info['id']\n",
    "\n",
    "    for ann in annotations:\n",
    "        if ann['image_id'] != image_id:\n",
    "            continue\n",
    "\n",
    "        category_id = ann['category_id']\n",
    "        if category_id not in category_map:\n",
    "            continue\n",
    "\n",
    "        category_value = category_map[category_id]\n",
    "        seg = ann.get('segmentation', [])\n",
    "\n",
    "        if not seg or not isinstance(seg, list):\n",
    "            continue\n",
    "\n",
    "        for poly in seg:\n",
    "            if len(poly) < 6:\n",
    "                continue  # invalid polygon\n",
    "            try:\n",
    "                poly = np.array(poly).reshape((-1, 2))\n",
    "                rr, cc = polygon(poly[:, 1], poly[:, 0], shape=mask.shape)\n",
    "                mask[rr, cc] = category_value\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing polygon: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Save with same name as image, but in mask folder\n",
    "    file_stem = os.path.splitext(image_info['file_name'])[0]\n",
    "    mask_filename = f\"{file_stem}.png\"\n",
    "    Image.fromarray(mask).save(os.path.join(mask_output_dir, mask_filename))\n",
    "\n",
    "def main(json_path, image_dir, mask_output_dir, limit=6000):\n",
    "    with open(json_path, 'r') as f:\n",
    "        coco = json.load(f)\n",
    "\n",
    "    os.makedirs(mask_output_dir, exist_ok=True)\n",
    "\n",
    "    images = coco['images'][:limit]\n",
    "    annotations = coco['annotations']\n",
    "    categories = coco['categories']\n",
    "\n",
    "    # category_id → consecutive integers starting from 1\n",
    "    category_map = {cat['id']: idx + 1 for idx, cat in enumerate(categories)}\n",
    "    print(f\"Category map: {category_map}\")\n",
    "\n",
    "    for image_info in tqdm(images, desc=\"Processing COCO images\"):\n",
    "        create_multiclass_mask(image_info, annotations, mask_output_dir, category_map)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    json_path = 'annotations/instances_train2017.json'\n",
    "    image_dir = 'train2017'\n",
    "    mask_output_dir = 'masks'\n",
    "    main(json_path, image_dir, mask_output_dir, limit=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.psi(F.relu(g1 + x1))\n",
    "        return x * psi\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_ch=3, out_ch=91):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.down1 = ConvBlock(in_ch, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.down2 = ConvBlock(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.down3 = ConvBlock(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.down4 = ConvBlock(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bridge = ConvBlock(512, 1024)\n",
    "\n",
    "        self.att4 = AttentionBlock(512, 512, 256)\n",
    "        self.up4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
    "        self.up_conv4 = ConvBlock(1024, 512)\n",
    "\n",
    "        self.att3 = AttentionBlock(256, 256, 128)\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.up_conv3 = ConvBlock(512, 256)\n",
    "\n",
    "        self.att2 = AttentionBlock(128, 128, 64)\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.up_conv2 = ConvBlock(256, 128)\n",
    "\n",
    "        self.att1 = AttentionBlock(64, 64, 32)\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.up_conv1 = ConvBlock(128, 64)\n",
    "\n",
    "        self.final = nn.Conv2d(64, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.down1(x)\n",
    "        p1 = self.pool1(c1)\n",
    "\n",
    "        c2 = self.down2(p1)\n",
    "        p2 = self.pool2(c2)\n",
    "\n",
    "        c3 = self.down3(p2)\n",
    "        p3 = self.pool3(c3)\n",
    "\n",
    "        c4 = self.down4(p3)\n",
    "        p4 = self.pool4(c4)\n",
    "\n",
    "        bridge = self.bridge(p4)\n",
    "\n",
    "        up4 = self.up4(bridge)\n",
    "        att4 = self.att4(g=up4, x=c4)\n",
    "        merge4 = torch.cat([up4, att4], dim=1)\n",
    "        c_up4 = self.up_conv4(merge4)\n",
    "\n",
    "        up3 = self.up3(c_up4)\n",
    "        att3 = self.att3(g=up3, x=c3)\n",
    "        merge3 = torch.cat([up3, att3], dim=1)\n",
    "        c_up3 = self.up_conv3(merge3)\n",
    "\n",
    "        up2 = self.up2(c_up3)\n",
    "        att2 = self.att2(g=up2, x=c2)\n",
    "        merge2 = torch.cat([up2, att2], dim=1)\n",
    "        c_up2 = self.up_conv2(merge2)\n",
    "\n",
    "        up1 = self.up1(c_up2)\n",
    "        att1 = self.att1(g=up1, x=c1)\n",
    "        merge1 = torch.cat([up1, att1], dim=1)\n",
    "        c_up1 = self.up_conv1(merge1)\n",
    "\n",
    "        return self.final(c_up1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOSegmentation(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_paths = sorted(glob(os.path.join(image_dir, \"*.jpg\")))\n",
    "        self.mask_paths = sorted(glob(os.path.join(mask_dir, \"*.png\")))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        mask = Image.open(self.mask_paths[idx])\n",
    "\n",
    "        image = np.array(image)\n",
    "        mask = np.array(mask, dtype=np.int64)\n",
    "\n",
    "        if self.transform:\n",
    "            aug = self.transform(image=image, mask=mask)\n",
    "            image = aug['image']\n",
    "            mask = aug['mask']\n",
    "\n",
    "        image = T.ToTensor()(image)\n",
    "        return image, torch.tensor(mask, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, device, epochs=5):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, masks in tqdm(dataloader):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3442 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Paths to your dataset (images and masks generated previously)\n",
    "image_dir = \"train2017\"\n",
    "mask_dir = \"masks\"\n",
    "\n",
    "# Transforms\n",
    "transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = COCOSegmentation(image_dir, mask_dir, transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = UNet(in_channels=3, num_classes=91).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 25\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for images, masks in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} loss: {epoch_loss / len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on a single batch\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, masks in dataloader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        break  # Show only first batch\n",
    "\n",
    "# Visualize results\n",
    "for i in range(4):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(images[i].permute(1, 2, 0).cpu())\n",
    "    plt.title(\"Input Image\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(masks[i].cpu())\n",
    "    plt.title(\"Ground Truth\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(preds[i])\n",
    "    plt.title(\"Prediction\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
